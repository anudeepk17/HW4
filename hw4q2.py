# -*- coding: utf-8 -*-
"""HW4Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xaXl2mOP46thKWhxVzuSzBHqvQ-aqxXp

Importing
"""

import numpy as np
import random
from collections import Counter
from sklearn.metrics import confusion_matrix

everychars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' ']
everylang = ['e','j','s']
Language={'e':'English','j':'Japanese','s':'Spanish'}

"""### Question 2"""

train_X = []
train_Y = []

for i,lang in enumerate(everylang):
    for N in range(10):
        with open(f"{lang}{N}.txt",'r+') as f:
            txt = f.readlines()
        txt = ''.join(txt)
        txt = Counter(txt)
        txt = [txt[i] for i in everychars]
        train_X.append(txt)
        train_Y.append(i)
train_X = np.asarray(train_X)
train_Y = np.asarray(train_Y)

all_english = train_X[train_Y==0]
count_english = np.sum(all_english,axis=0)
count_english = count_english+0.5
prob_eng = count_english/sum(count_english)
print(prob_eng)

"""### Question 3"""

all_japan = train_X[train_Y==1]
count_japan = np.sum(all_japan,axis=0)
count_japan = count_japan+0.5
prob_jap = count_japan/sum(count_japan)
print(prob_jap)

all_spain = train_X[train_Y==2]
count_spain = np.sum(all_spain,axis=0)
count_spain = count_spain+0.5
prob_spa = count_spain/sum(count_spain)
print(prob_spa)

"""### Question 4"""

with open("e10.txt",'r+') as f:
    txt = f.readlines()
    txt = ''.join(txt)
    txt = Counter(txt)

bag_e10 = [txt[i] for i in everychars]
print(bag_e10)

"""### Question 5 and 6"""

p_eng = sum(bag_e10*np.log(prob_eng))
p_spa = sum(bag_e10*np.log(prob_spa))
p_jap = sum(bag_e10*np.log(prob_jap))
print(p_eng, p_jap, p_spa)
print(f"Predicted class is \'{Language[everylang[np.argmax([p_eng, p_jap, p_spa])]]}\'")

"""### Question 7"""

test_X = []
test_Y = []
for i,lang in enumerate(['e','j','s']):
    for N in range(10,20):
        with open(f"{lang}{N}.txt",'r+') as f:
            txt = f.readlines()
        txt = ''.join(txt)
        txt = Counter(txt)
        txt = [txt[i] for i in everychars]
        test_X.append(txt)
        test_Y.append(i)
test_X = np.asarray(test_X)
test_Y = np.asarray(test_Y)

y_pred = []
for x in test_X:
    p_eng = sum(x*np.log(prob_eng))
    p_spa = sum(x*np.log(prob_spa))
    p_jap = sum(x*np.log(prob_jap))
    y_pred.append(np.argmax([p_eng, p_jap, p_spa]))

print("Confusion Matrix")
print("\tEng\tJap\tSpa")
confmat = confusion_matrix(test_Y,y_pred)
print(f"Eng\t{confmat[0][0]}\t{confmat[0][1]}\t{confmat[0][2]}")
print(f"Jap\t{confmat[1][0]}\t{confmat[1][1]}\t{confmat[1][2]}")
print(f"Spa\t{confmat[2][0]}\t{confmat[2][1]}\t{confmat[2][2]}")

"""### Question 8"""

with open("j6.txt",'r+') as f:
    txt = f.readlines()
    txt = ''.join(txt)
    print("===Original Text===")
    print(txt)
    temp = list(txt)
    random.shuffle(temp)
    txt = ''.join(temp)
    print("===Shuffeled Text===")
    print(txt)
    txt = Counter(txt)
print("\n====================\n")
bag_e10 = [txt[i] for i in everychars]

p_eng = sum(bag_e10*np.log(prob_eng))
p_spa = sum(bag_e10*np.log(prob_spa))
p_jap = sum(bag_e10*np.log(prob_jap))
print(p_eng, p_jap, p_spa)
print(f"Predicted class is \'{Language[everylang[np.argmax([p_eng, p_jap, p_spa])]]}\'")